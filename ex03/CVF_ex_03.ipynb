{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "CVF_ex_03.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFXTdyBfvabT",
        "colab_type": "text"
      },
      "source": [
        "### Working with Google Colaboratory\n",
        "If you decide to work with Google Colaboratory (recommended because it comes with `pyTorch` installed and the possibility to use GPU for computations), you need to run the next code block before you start working with the notebook.\n",
        "\n",
        "To get a better idea of what the code below is doing, have a look at this [introductory video](https://cs231n.github.io/setup-instructions/) from the CS231n Standford course.\n",
        "\n",
        "If you need to install additional packages on Google Colaboratory (it should not needed for this assignment) you can do it from the notebook with `!pip install name_package`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpYXNMurvabU",
        "colab_type": "code",
        "outputId": "452156c5-7128-4f83-e74b-e271ee2c34df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# enter the foldername in your Drive where you have saved the material for this assignment,\n",
        "# e.g. 'cvf20/assignments/assignment3/'\n",
        "FOLDERNAME = \"CVF/ex03/\"\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Make sure that the python modules in the assignment folder are found by the notebook:\n",
        "import sys\n",
        "import os\n",
        "path_drive = os.path.join(\"/content/drive/My\\ Drive\", FOLDERNAME)\n",
        "sys.path.append(path_drive)\n",
        "\n",
        "# Copy the yeast-cells data in the content folder of the notebook:\n",
        "dataset_path = os.path.join(path_drive, \"yeast_cells_dataset\") \n",
        "!cp -r $dataset_path ./\n",
        "\n",
        "# Move to the main content folder:\n",
        "%cd /content\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGOUOBa1vabZ",
        "colab_type": "text"
      },
      "source": [
        "# 1. Parameters and Receptive Field of a CNN\n",
        "*Insert your answer here (or in a seperate `.pdf` file).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w51elSqL3Ci_",
        "colab_type": "code",
        "outputId": "da951a1c-4c1b-4de8-9dec-4a315b5c9e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "print(\"\\nPart (a)\\n\")\n",
        "\n",
        "c1 = (5*5*3)*32\n",
        "c2 = (3*3*32)*32\n",
        "c3 = (3*3*32)*64\n",
        "c4 = (3*3*64)*64\n",
        "c5 = (3*3*64)*128\n",
        "c_total = c1+c2+c3+c4+c5\n",
        "print(\"Total number of parameters in 5 convolutional layers =\",c_total)\n",
        "f1 = (14*14*128)*512\n",
        "f2 = 512 * 10\n",
        "f_total = f1+f2\n",
        "print(\"Total number of parameters in 2 fully connected layers =\",f_total)\n",
        "print(\"No parameters required in ReLu, MaxPool and Softmax since they are all parameter-free activation functions.\")\n",
        "print(\"Hence, the total number of learnable parameters in the entire architecture is the sum of parameters in the convolutional layers and the fully connected layers. \\nAns =\",c_total+f_total)\n",
        "\n",
        "\n",
        "print(\"\\nPart (b)\\n\")\n",
        "\n",
        "c_output = 14*14*128\n",
        "print(\"Output of the convolutional part of the model (14*14*128) = \", c_output)\n",
        "rf_theoretical = (14+(2*1)-3+1)*(14+(2*1)-3+1)\n",
        "print(\"Theoretical receptive field before fully connected layers = \",rf_theoretical)\n",
        "print(\"\\nIn our model, the convolution output is assumed to have the same spatial dimension as input image (56*56) since the input is always padded with zeros but we had two max pool filters and hence the dimensions were reduced by a factor of 4, hence (14*14) \\n\"\n",
        "\"By using the formula d_in+2*padding-kernel_size+1 we get the value as 14\"\n",
        "\"Furthermore, The receptive field of a fully connected network is the entire dimension of the input and hence(14*14) \\n\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Part (a)\n",
            "\n",
            "Total number of parameters in 5 convolutional layers = 140640\n",
            "Total number of parameters in 2 fully connected layers = 12850176\n",
            "No parameters required in ReLu, MaxPool and Softmax since they are all parameter-free activation functions.\n",
            "Hence, the total number of learnable parameters in the entire architecture is the sum of parameters in the convolutional layers and the fully connected layers. \n",
            "Ans = 12990816\n",
            "\n",
            "Part (b)\n",
            "\n",
            "Output of the convolutional part of the model (3*3*128) =  1152\n",
            "Theoretical receptive field before fully connected layers =  196\n",
            "\n",
            "In our model, the convolution output is assumed to have the same spatial dimension as input image (56*56) since the input is always padded with zeros but we had two max pool filters and hence the dimensions were reduced by a factor of 4, hence (14*14) \n",
            "By using the formula d_in+2*padding-kernel_size+1 we get the value as 14Furthermore, The receptive field of a fully connected network is the entire dimension of the input and hence(14*14) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Y2nKmGPGvaba",
        "colab_type": "text"
      },
      "source": [
        "# 2. Train a CNN for Semantic Segmentation (Part 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lWIZt3j_vaba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The usual first imports:\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import numpy as np\n",
        "import scipy.ndimage\n",
        "plt.rcParams['figure.figsize'] = [15, 15]\n",
        "\n",
        "import torch\n",
        "import h5py\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoTVRJbEvabe",
        "colab_type": "text"
      },
      "source": [
        "### a) Loading the data\n",
        "You can find the data in the `yeast_cells_dataset` folder, including both the input images and associated ground labels. In this first part of the exercise, we want to create a single file called `yeast_cells_dataset/dataset.hdf5` with the following specifications:\n",
        "\n",
        "- the HDF5 file should have two internal datasets called `raw` and `gt` (see a quick introduction [here](http://docs.h5py.org/en/stable/quick.html) if you have never used the library `h5py`)\n",
        "- each dataset should have 3 dimensions with shape (nb_images, x_shape, y_shape) = (18, 920, 1760): the first dimension represents the image index (there are 18 images in total), whereas the second and third ones are equal to the resolution of the images.\n",
        "- When you create the datasets with the h5py `create_dataset()` function, make sure to use the argument `compression='gzip'` so that the resulting file has a small size\n",
        "\n",
        "Implement the function below and then test it with `check_dataset()`.\n",
        "\n",
        "**Important**: if the `.hdf5` files you create are too big, you don't need to include them in your submission (same for the folder with the `.png` images). However, your notebook should show all your results and your plots. If we need to re-run your notebook, we will make sure that the original `yeast_cells_dataset` folder with the `.png` images is found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false,
        "id": "D8KcU1QSvabf",
        "colab_type": "code",
        "outputId": "961d890f-96e8-46bd-8f7d-230702e67323",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def create_hdf5_dataset():\n",
        "    ### Start of your code ###\n",
        "\n",
        "    prefix = './yeast_cells_dataset/'\n",
        "    with h5py.File(prefix+\"dataset.hdf5\", \"w\") as f:\n",
        "      for j in ['raw','gt']:\n",
        "        dset = f.create_dataset(j, (18,920,1760), compression=\"gzip\")\n",
        "        for i in range(18):\n",
        "          dset[i,:,:] = plt.imread(prefix+j+\"_{}.png\".format(i))\n",
        "\n",
        "    ### End of your code ###\n",
        "\n",
        "\n",
        "def check_dataset():\n",
        "    path = './yeast_cells_dataset/dataset.hdf5'\n",
        "    assert os.path.exists(path), \"Dataset does not exists\"\n",
        "    \n",
        "    with h5py.File(path, 'r') as f:\n",
        "        assert \"raw\" in f, \"Raw dataset not found in .hdf5 file\"\n",
        "        assert \"gt\" in f, \"GT dataset not found in .hdf5 file\"\n",
        "        assert f[\"raw\"].ndim == 3, \"Wrong nb of dimensions\"\n",
        "        assert f[\"gt\"].ndim == 3, \"Wrong nb of dimensions\"\n",
        "        assert f[\"raw\"].shape == (18, 920, 1760), \"Wrong shape\"\n",
        "        assert f[\"gt\"].shape == (18, 920, 1760), \"Wrong shape\"\n",
        "        \n",
        "    print(\"Hdf5 dataset correctly created!\")\n",
        "\n",
        "        \n",
        "create_hdf5_dataset()\n",
        "check_dataset()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hdf5 dataset correctly created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoU6I6jDvabj",
        "colab_type": "text"
      },
      "source": [
        "### b) Normalization\n",
        "Complete the code below to normalize the dataset and after doing it print the mean and the standard deviation before and after normalizing it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRitsdGCvabk",
        "colab_type": "code",
        "outputId": "9c148c9b-8bb3-4cdf-d62f-4dcdf1b9bd95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def normalize_hdf5_dataset():\n",
        "    \"\"\"\n",
        "    TODO: print the mean and the standard deviation before and after normalizing the raw data\n",
        "    \"\"\"\n",
        "    original_path = './yeast_cells_dataset/dataset.hdf5'\n",
        "    new_path = './yeast_cells_dataset/dataset_normalized.hdf5'\n",
        "    \n",
        "    ### Start of your code ###\n",
        "    original = h5py.File(original_path,\"r\")\n",
        "\n",
        "    with h5py.File(new_path, \"w\") as new:\n",
        "      for j in ['raw','gt']:\n",
        "        dset = new.create_dataset(j, (18,920,1760), compression=\"gzip\")\n",
        "        for i in range(18):\n",
        "          mean = original[j][i,:,:].mean()\n",
        "          sigma = original[j][i,:,:].std()\n",
        "          print(\"original_\"+j+\"_{}, mean={},sigma={}\".format(i,round(mean,1),round(sigma,1)))\n",
        "          new[j][i,:,:] = (original[j][i,:,:] - mean)/sigma\n",
        "          mean_new = new[j][i,:,:].mean()\n",
        "          sigma_new = new[j][i,:,:].std()\n",
        "          print(\"normalized_\"+j+\"_{}, mean={},sigma={}\".format(i,round(mean_new,1),round(sigma_new,1)))\n",
        "\n",
        "    ### End of your code ###\n",
        "\n",
        "normalize_hdf5_dataset()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original_raw_0, mean=0.4000000059604645,sigma=0.0\n",
            "normalized_raw_0, mean=-0.0,sigma=1.0\n",
            "original_raw_1, mean=0.4000000059604645,sigma=0.10000000149011612\n",
            "normalized_raw_1, mean=-0.0,sigma=1.0\n",
            "original_raw_2, mean=0.4000000059604645,sigma=0.10000000149011612\n",
            "normalized_raw_2, mean=0.0,sigma=1.0\n",
            "original_raw_3, mean=0.4000000059604645,sigma=0.0\n",
            "normalized_raw_3, mean=-0.0,sigma=1.0\n",
            "original_raw_4, mean=0.4000000059604645,sigma=0.10000000149011612\n",
            "normalized_raw_4, mean=0.0,sigma=1.0\n",
            "original_raw_5, mean=0.4000000059604645,sigma=0.10000000149011612\n",
            "normalized_raw_5, mean=-0.0,sigma=1.0\n",
            "original_raw_6, mean=0.4000000059604645,sigma=0.0\n",
            "normalized_raw_6, mean=0.0,sigma=1.0\n",
            "original_raw_7, mean=0.4000000059604645,sigma=0.10000000149011612\n",
            "normalized_raw_7, mean=-0.0,sigma=1.0\n",
            "original_raw_8, mean=0.4000000059604645,sigma=0.10000000149011612\n",
            "normalized_raw_8, mean=0.0,sigma=1.0\n",
            "original_raw_9, mean=0.4000000059604645,sigma=0.10000000149011612\n",
            "normalized_raw_9, mean=-0.0,sigma=1.0\n",
            "original_raw_10, mean=0.4000000059604645,sigma=0.10000000149011612\n",
            "normalized_raw_10, mean=-0.0,sigma=1.0\n",
            "original_raw_11, mean=0.4000000059604645,sigma=0.10000000149011612\n",
            "normalized_raw_11, mean=0.0,sigma=1.0\n",
            "original_raw_12, mean=0.4000000059604645,sigma=0.0\n",
            "normalized_raw_12, mean=-0.0,sigma=1.0\n",
            "original_raw_13, mean=0.4000000059604645,sigma=0.10000000149011612\n",
            "normalized_raw_13, mean=-0.0,sigma=1.0\n",
            "original_raw_14, mean=0.4000000059604645,sigma=0.10000000149011612\n",
            "normalized_raw_14, mean=0.0,sigma=1.0\n",
            "original_raw_15, mean=0.4000000059604645,sigma=0.0\n",
            "normalized_raw_15, mean=0.0,sigma=1.0\n",
            "original_raw_16, mean=0.4000000059604645,sigma=0.10000000149011612\n",
            "normalized_raw_16, mean=0.0,sigma=1.0\n",
            "original_raw_17, mean=0.4000000059604645,sigma=0.10000000149011612\n",
            "normalized_raw_17, mean=-0.0,sigma=1.0\n",
            "original_gt_0, mean=0.4000000059604645,sigma=0.5\n",
            "normalized_gt_0, mean=0.0,sigma=1.0\n",
            "original_gt_1, mean=0.4000000059604645,sigma=0.5\n",
            "normalized_gt_1, mean=-0.0,sigma=1.0\n",
            "original_gt_2, mean=0.4000000059604645,sigma=0.5\n",
            "normalized_gt_2, mean=0.0,sigma=1.0\n",
            "original_gt_3, mean=0.4000000059604645,sigma=0.5\n",
            "normalized_gt_3, mean=0.0,sigma=1.0\n",
            "original_gt_4, mean=0.4000000059604645,sigma=0.5\n",
            "normalized_gt_4, mean=-0.0,sigma=1.0\n",
            "original_gt_5, mean=0.4000000059604645,sigma=0.5\n",
            "normalized_gt_5, mean=0.0,sigma=1.0\n",
            "original_gt_6, mean=0.30000001192092896,sigma=0.5\n",
            "normalized_gt_6, mean=0.0,sigma=1.0\n",
            "original_gt_7, mean=0.4000000059604645,sigma=0.5\n",
            "normalized_gt_7, mean=-0.0,sigma=1.0\n",
            "original_gt_8, mean=0.4000000059604645,sigma=0.5\n",
            "normalized_gt_8, mean=0.0,sigma=1.0\n",
            "original_gt_9, mean=0.30000001192092896,sigma=0.5\n",
            "normalized_gt_9, mean=-0.0,sigma=1.0\n",
            "original_gt_10, mean=0.4000000059604645,sigma=0.5\n",
            "normalized_gt_10, mean=-0.0,sigma=1.0\n",
            "original_gt_11, mean=0.4000000059604645,sigma=0.5\n",
            "normalized_gt_11, mean=0.0,sigma=1.0\n",
            "original_gt_12, mean=0.4000000059604645,sigma=0.5\n",
            "normalized_gt_12, mean=-0.0,sigma=1.0\n",
            "original_gt_13, mean=0.5,sigma=0.5\n",
            "normalized_gt_13, mean=0.0,sigma=1.0\n",
            "original_gt_14, mean=0.5,sigma=0.5\n",
            "normalized_gt_14, mean=0.0,sigma=1.0\n",
            "original_gt_15, mean=0.4000000059604645,sigma=0.5\n",
            "normalized_gt_15, mean=0.0,sigma=1.0\n",
            "original_gt_16, mean=0.4000000059604645,sigma=0.5\n",
            "normalized_gt_16, mean=0.0,sigma=1.0\n",
            "original_gt_17, mean=0.5,sigma=0.5\n",
            "normalized_gt_17, mean=-0.0,sigma=1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL29rcZZvabn",
        "colab_type": "text"
      },
      "source": [
        "### c) Data augmentation\n",
        "**Task 1** -- We first want to create a `PyTorch` dataset from the data we loaded previously, using the `torch.utils.data.dataset` class.\n",
        "\n",
        "When we train our CNN model, we want to first crop the yeast-cells images to a fixed shape of $(512x512)$. The classes `ImagesFromH5File` and `YeastCellDataset` (see the `cvf20/datasets.py` file) are exactly doing that and expects the path to a `.hdf5` file like the one you created previously. \n",
        "\n",
        "Read the documentation of the `YeastCellDataset` class (see `cvf20/datasets.py`) and create the PyTorch dataset in the code below. As `stride`, you can use (32, 32)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false,
        "id": "fY3Aw_VJvabo",
        "colab_type": "code",
        "outputId": "7a61540e-a975-4bde-a822-5b1c17c67062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/CVF/ex03\n",
        "%cd /content\n",
        "\n",
        "from cvf20.datasets import YeastCellDataset\n",
        "\n",
        "### Start of your code ###\n",
        "# dataset = YeastCellDataset(...)\n",
        "# dataset = None\n",
        "\n",
        "dataset = YeastCellDataset(path_h5_file='./yeast_cells_dataset/dataset_normalized.hdf5', window_size = (512,512), stride=(32, 32), mode=\"train\", transforms=None)\n",
        "\n",
        "### End of your code ###\n",
        "\n",
        "\n",
        "# Validate your code and plot some images:\n",
        "if dataset is not None:\n",
        "    assert dataset[0][0].shape == (512, 512)\n",
        "\n",
        "    f, ax = plt.subplots(ncols=2, nrows=2)\n",
        "    ax[0,0].imshow(dataset[0][0], cmap='gray')\n",
        "    ax[0,1].imshow(dataset[0][1], cmap='gray')\n",
        "    ax[1,0].imshow(dataset[1][0], cmap='gray')\n",
        "    ax[1,1].imshow(dataset[1][1], cmap='gray')\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CVF/ex03\n",
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-829b81009841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd /content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvf20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYeastCellDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m### Start of your code ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cvf20'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2PaqMT17pAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyFEOAzWvabr",
        "colab_type": "text"
      },
      "source": [
        "**Task 2** -- Now that you created the dataset, let's add some more data augmentation to it!\n",
        "\n",
        "In the `cvf20/transforms.py` file you can find an example of transformation you should implement: the class `DownscaleImage` is, as you would expect, downscaling an image by a given factor (by first applying a filter to it). Tranformations are usually not applied to all images in the dataset, but they are randomly applied with a certain probability. In order to understand the functionality of the `build_random_variables()` method you can have a look at the parent tranform class `BasicTransform2D` (also in `cvf20/transforms.py`).\n",
        "\n",
        "In the code block below, implement two additional transformations that should: i) reflect an image along x and/or along y with probability 0.5; ii) randomly rotate an image of a multiple of 90 degrees. In the next code block, you can plot some images and check if your transformations are applied correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgO09WCKvabr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cvf20.transforms as T\n",
        "\n",
        "class RandomFlip(T.BasicTransform2D):\n",
        "    def build_random_variables(self):\n",
        "        ### Start of your code ###\n",
        "        self._random_variables = {\n",
        "            \"apply_flip_x\": np.random.random()<0.5,\n",
        "            \"apply_flip_y\": np.random.random()<0.5\n",
        "        }\n",
        "        ### End of your code ###\n",
        "        \n",
        "    def apply_transform_to_image(self, image):\n",
        "        assert image.ndim == 2\n",
        "        ### Start of your code ###\n",
        "        if self.get_random_variable(\"apply_flip_x\"):\n",
        "            image = np.flip(image,0)\n",
        "        if self.get_random_variable(\"apply_flip_y\"):\n",
        "            image = np.flip(image,1)\n",
        "        ### End of your code ###\n",
        "        return image\n",
        "\n",
        "\n",
        "class RandomRotation(T.BasicTransform2D):\n",
        "    def build_random_variables(self):\n",
        "        ### Start of your code ###\n",
        "        self._random_variables = {\n",
        "            \"apply_rotation\": np.random.randint(0,4)\n",
        "        }\n",
        "        ### End of your code ###\n",
        "\n",
        "    def apply_transform_to_image(self, image):\n",
        "        assert image.ndim == 2\n",
        "        ### Start of your code ###\n",
        "        n = self.get_random_variable(\"apply_rotation\")\n",
        "        image = np.rot90(image,n)\n",
        "        ### End of your code ###\n",
        "        return image\n",
        "    \n",
        "all_transforms = T.Compose(\n",
        "    RandomFlip(),\n",
        "    RandomRotation()\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyME9k9PJ07a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = np.arange(16).reshape((4,4))\n",
        "print(a)\n",
        "a= np.flip(a,0)\n",
        "print(a)\n",
        "a = np.rot90(a,1)\n",
        "print(a)\n",
        "\n",
        "dataset[0][0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEo2NujQvabv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from cvf20.datasets import YeastCellDataset\n",
        "\n",
        "### Start of your code ###\n",
        "#dataset_with_transforms = None\n",
        "dataset_with_transforms = YeastCellDataset(path_h5_file='./yeast_cells_dataset/dataset_normalized.hdf5', window_size = (512,512), stride=(32, 32), mode=\"train\", transforms=all_transforms)\n",
        "\n",
        "### End of your code ###\n",
        "\n",
        "# Validate your code and plot some images:\n",
        "if dataset_with_transforms is not None:\n",
        "    assert dataset_with_transforms[0][0].shape == (512, 512)\n",
        "\n",
        "    f, ax = plt.subplots(ncols=4)\n",
        "    ax[0].imshow(dataset_with_transforms[0][0], cmap='gray')\n",
        "    ax[1].imshow(dataset_with_transforms[0][0], cmap='gray')\n",
        "    ax[2].imshow(dataset_with_transforms[0][0], cmap='gray')\n",
        "    ax[3].imshow(dataset_with_transforms[0][0], cmap='gray')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEQN2o0Nvaby",
        "colab_type": "text"
      },
      "source": [
        "**Task 3** -- Finally, we will now create a PyTorch `DataLoader` that will take care of randomly selecting some of the crops in the dataset. The shape of the PyTorch tensors that we will feed to the neural network is the following: `(batch_size, nb_channels, x_size, y_size)`. The dimension of the tensor is 4 because we are dealing with 2D images.  `nb_channels` for the input image is 1, because it has only one channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqinUgSuvaby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "all_transforms = T.Compose(\n",
        "    RandomFlip(),\n",
        "    RandomRotation(),\n",
        "    T.ToTorchTensor()\n",
        ")\n",
        "\n",
        "### Start of your code ###\n",
        "dataset_with_transforms = YeastCellDataset(path_h5_file='./yeast_cells_dataset/dataset_normalized.hdf5', window_size = (512,512), stride=(32, 32), mode=\"train\", transforms=all_transforms)\n",
        "### End of your code ###\n",
        "\n",
        "# Create the data loader:\n",
        "assert dataset_with_transforms is not None\n",
        "dataset_loader = DataLoader(\n",
        "        dataset_with_transforms, \n",
        "        batch_size=4,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=2\n",
        ")\n",
        "\n",
        "# Validate your code:\n",
        "for raw, gt in dataset_loader:\n",
        "    assert isinstance(raw, torch.Tensor)\n",
        "    assert raw.shape == (4,1,512,512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94kudCNQvab3",
        "colab_type": "text"
      },
      "source": [
        "### d) Semantic Segmentation Metrics\n",
        "In the code below, implement the two metrics described in the exercise sheet. After implementing it, run the next code block to test your functions on a small 5x5 image.\n",
        "\n",
        "*Remark*: now the images (prediction of the CNN model and ground truth labels) are `PyTorch` tensors and not `numpy` arrays. Most of the functions in `PyTorch` to perform matrix and vector operations are very similar to the ones you used in `numpy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KSLBgQ3vab4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import jaccard_score as jc\n",
        "\n",
        "def compute_accuracy(predictions, targets):\n",
        "    \"\"\"\n",
        "    The shape of both `predictions` and `targets` should be (batch_size, nb_classes, x_size_image, y_size_image)\n",
        "    \"\"\"\n",
        "    assert isinstance(predictions, torch.Tensor)\n",
        "    assert isinstance(targets, torch.Tensor)\n",
        "    assert predictions.ndim == 4 and targets.ndim == 4\n",
        "    \n",
        "    ### Start of your code ###\n",
        "    accuracy = None\n",
        "    equal_pixels=torch.eq(predictions, targets)\n",
        "    count_equal=0\n",
        "    count_total= 2*5*5\n",
        "    myTensor = tf.where(equal_pixels)\n",
        "    count_equal=tf.shape(myTensor)[0]\n",
        "    accuracy = count_equal/count_total\n",
        "    ### End of your code ###\n",
        "    \n",
        "    return accuracy\n",
        "    \n",
        "\n",
        "def compute_IoU(predictions, targets):\n",
        "    \"\"\"\n",
        "    The shape of both `predictions` and `targets` should be (batch_size, nb_classes, x_size_image, y_size_image)\n",
        "    \"\"\"\n",
        "    assert isinstance(predictions, torch.Tensor)\n",
        "    assert isinstance(targets, torch.Tensor)\n",
        "    assert predictions.ndim == 4 and targets.ndim == 4\n",
        "    \n",
        "    ### Start of your code ###\n",
        "    IoU = None\n",
        "    \n",
        "    intersection = torch.logical_and(predictions,targets)\n",
        "    union = torch.logical_or(predictions,targets)\n",
        "    myTensor1 = tf.where(intersection)\n",
        "    count_intersection=tf.shape(myTensor1)[0]\n",
        "    myTensor2 = tf.where(union)\n",
        "    count_union=tf.shape(myTensor2)[0]\n",
        "    IoU = count_intersection/count_union \n",
        "    ### End of your code ###\n",
        "    \n",
        "    return IoU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh4thdAdvab7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a small 5x5 example to test the metrics above:\n",
        "test_pred, test_gt = torch.zeros((2,5,5)), torch.zeros((2,5,5))\n",
        "test_pred[0,0,:3] = 0.8\n",
        "test_gt[0,0,2:] = 1\n",
        "test_pred[1] = 1. - test_pred[0]\n",
        "test_gt[1] = 1. - test_gt[0]\n",
        "test_pred = test_pred.unsqueeze(0)\n",
        "test_gt = test_gt.unsqueeze(0)\n",
        "\n",
        "print(compute_accuracy(test_pred, test_gt))\n",
        "print(compute_IoU(test_pred, test_gt))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0HkOSNgkJNr",
        "colab_type": "text"
      },
      "source": [
        "In my opinion IoU is more informative since we have only 2 classes here: background and foreground, the IoU score will give us the overlap between our prediction and ground truth. If the IoU is high it would mean that our prediction is good. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcpKwyoPVFtk",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}
