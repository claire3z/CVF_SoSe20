{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ex_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E6ifDrqZVcgN"
      },
      "source": [
        "### Requirements and Google Colab setup\n",
        "The requirements that you will need for this assigment are:\n",
        "\n",
        "- If you work on Google Colab:\n",
        "    - tensorboardX\n",
        "    - a small utility called `ngrok` that let you see the Tensorboard panel in a separate webpage\n",
        "- If you use your local GPU with your local environment:\n",
        "    - (PyTorch)\n",
        "    - tensorboardX\n",
        "    - tensorboard (included if you already have tensorflow)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3Jzp1zwFVcgO",
        "outputId": "4c38afd2-2333-407e-fff3-4900a048acd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "#### RUN THIS CODE IF YOU USE GOOGLE COLAB ####\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# enter the foldername in your Drive where you have saved the material for this assignment,\n",
        "# e.g. 'cvf20/assignments/assignment3/'\n",
        "FOLDERNAME = 'CVF/ex04/'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Make sure that the python modules in the assignment folder are found by the notebook:\n",
        "import sys\n",
        "import os\n",
        "path_drive = os.path.join(\"/content/drive/My Drive\", FOLDERNAME)\n",
        "sys.path.append(path_drive)\n",
        "\n",
        "# Copy the yeast-cells data in the content folder of the notebook:\n",
        "dataset_path = os.path.join(path_drive, \"yeast_cells_dataset\") \n",
        "dataset_path = dataset_path.replace(\" \", \"\\ \")\n",
        "!cp -r $dataset_path ./\n",
        "\n",
        "# Move to the main content folder:\n",
        "%cd /content\n",
        "\n",
        "# Install tensorboardX:\n",
        "!pip install tensorboardX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (46.3.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbdilwjLt_-a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "2ef779f7-257e-448f-9d5c-2ddb6f8e9c9a"
      },
      "source": [
        "#### RUN THIS CODE IF YOU USE GOOGLE COLAB OR IF YOU WANT TO USE THE NGROK UTILITY ####\n",
        "\n",
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-26 21:46:37--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.86.203.217, 54.159.115.94, 50.17.2.180, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.86.203.217|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  6.51MB/s    in 2.0s    \n",
            "\n",
            "2020-05-26 21:46:40 (6.51 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7x3Grbh-Vcgg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# 1. Train a CNN for Semantic Segmentation (Part 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t_s-s4liVcgi",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {}
      },
      "source": [
        "# The usual imports:\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import numpy as np\n",
        "import scipy.ndimage\n",
        "plt.rcParams['figure.figsize'] = [15, 15]\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import h5py\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LKWnguALVcgr"
      },
      "source": [
        "### a) Loss functions for semantic segmentation\n",
        "In the code block below, implement the Dice loss defined in the description of the assignment. We will implement it as a subclass of `torch.nn.Module`, which is the base PyTorch class for all neural network modules. The method `forward()` is the one that performs the forward step, i.e. computes the output of the layer from the given inputs. \n",
        "\n",
        "We don't need to implement Binary Cross Entropy, since it is already implemented in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ycJ1lcoyVcgs",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "class SorensenDiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes a loss scalar, which when minimized maximizes the Sorensen-Dice similarity\n",
        "    between the input and the target.\n",
        "    \"\"\"\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super(SorensenDiceLoss, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "        input:      torch.FloatTensor with float values between 0 and 1\n",
        "        target:     torch.FloatTensor with binary values 0 and 1\n",
        "\n",
        "        Shape of the inputs: (batch_size, 1, x_size_image, y_size_image)\n",
        "        \n",
        "        When you divide by the denominator in the Dice loss formula, you can use the `eps` parameter and the\n",
        "        `clamp` method to avoid a division by zero:\n",
        "        \n",
        "         loss = 1 - 2 * (numerator / denominator.clamp(min=self.eps))\n",
        "        \n",
        "        \"\"\"\n",
        "        assert input.shape == target.shape\n",
        "        loss = torch.zeros((1,))\n",
        "        \n",
        "        ### Your code starts here:\n",
        "      \n",
        "        p = input\n",
        "        p_head = target\n",
        "        N = input.shape[0]\n",
        "        numerator = (p * p_head).sum()\n",
        "        denominator = (p*p).sum()+(p_head*p_head).sum() #NOTE: formular corrected.\n",
        "        loss = (1 - 2 * (numerator / denominator.clamp(min=self.eps)))/N\n",
        "      \n",
        "        ### Your code ends here\n",
        "        return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MmrLxhdaVcgy",
        "outputId": "8ebcaf7b-03f5-4a93-bb54-ac477b2bd269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test your implementation:\n",
        "test_pred, test_gt = torch.zeros((1,1,5,5)), torch.zeros((1,1,5,5))\n",
        "test_pred[0,0,0,:3] = 0.8\n",
        "test_gt[0,0,0,2:] = 1\n",
        "\n",
        "loss = SorensenDiceLoss()\n",
        "\n",
        "if np.allclose(loss(test_pred, test_gt).item(), 0.67479676):\n",
        "    print(\"Your implementation is correct!\")\n",
        "else:\n",
        "    print(\"There is some problem in your implementation\")\n",
        "    print(\"loss=\",loss(test_pred, test_gt).item())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your implementation is correct!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pFGRkrDwVcg2"
      },
      "source": [
        "### b) Training a UNet model\n",
        "Some information about the code that is provided:\n",
        "\n",
        "- In `cvf20/transforms.py` and `cvf20/metrics.py` you can find the data augmentation functions and the metrics that you implemented in the last assignment.\n",
        "- In `cvf20/utils.py` you can find a function to normalize the data. We will use the first 14 images for training and the last 4 for validation.\n",
        "- The implementation of the `UNet` model is in `cvf20/models/UNet.py`. In the code block below you find an example of basic UNet model with depth 5 and the correct number of input/output channels needed for our foreground/background task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nCFgWqHZVcg3"
      },
      "source": [
        "##### Task 1.) Creating the data loaders\n",
        "First, let's create the data loaders as we did in the last assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YmtL6bzqVcg3",
        "colab": {}
      },
      "source": [
        "from cvf20.utils import normalize_dataset\n",
        "from cvf20.datasets import YeastCellDataset\n",
        "import cvf20.transforms as T\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "# Compose and normalize the data in a .hdf5 file:\n",
        "normalize_dataset()\n",
        "\n",
        "# Add the transformations we used last time:\n",
        "all_transforms = T.Compose(\n",
        "    T.RandomFlip(),\n",
        "    T.RandomRotation(),\n",
        "    T.ToTorchTensor()\n",
        ")\n",
        "\n",
        "# For training, we choose a stride = (64,64). In this way during an epoch the same portion of an image are\n",
        "# seen multiple times, but we make sure that some parts are not always feeded at the border of the \n",
        "# training (512,512) window. \n",
        "# In your experiments you can tweak this parameter (smaller value equal to more iterations in one epoch):\n",
        "train_dataset = YeastCellDataset('./yeast_cells_dataset/dataset.hdf5',\n",
        "                          (512,512),\n",
        "                          (64,64),\n",
        "                          mode=\"train\",\n",
        "                          transforms=all_transforms\n",
        "                        )\n",
        "\n",
        "# For validation, we make sure to visit the data only once (so we set stride=(512,512)):\n",
        "val_dataset = YeastCellDataset('./yeast_cells_dataset/dataset.hdf5',\n",
        "                          (512,512),\n",
        "                          (512,512), \n",
        "                          mode=\"val\",\n",
        "                          transforms=all_transforms\n",
        "                        )\n",
        "\n",
        "# Create the data loaders:\n",
        "train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=4,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=2\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=4,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=2\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xofHCumsVcg6"
      },
      "source": [
        "##### Task 2.) Using a GPU\n",
        "Then, let's check if CUDA is available, i.e. if we can train on a GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mWZvoX2pVcg6",
        "outputId": "e14ecce4-a47f-47eb-a312-0adc03750610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# We will be using float throughout this tutorial\n",
        "dtype = torch.float32 \n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"Warning, GPU not available! Please make sure to use one otherwise the training will be VERY slow\")\n",
        "\n",
        "print('Using device:', device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5GTZy66SVcg8"
      },
      "source": [
        "##### Task 3.) The Trainer Class\n",
        "In the next code block you will find the `Trainer` class that you will use to train your model. It includes code to perform the training iterations, compute the loss, update the parameters in the neural network, evaluate the metrics and log data during training.\n",
        "\n",
        "Considering the goal of this exercise, it is not strictly needed to understand every line of code in it. But if you are eager to learn more, the code below will give you a good idea of how the PyTorch mechanics work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YR3cSdTsVcg8",
        "colab": {}
      },
      "source": [
        "from cvf20.metrics import compute_accuracy, compute_IoU\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, experiment_name, model,\n",
        "                 optimizer, loss_function,\n",
        "                 loader_train, loader_val,\n",
        "                 dtype, device,\n",
        "                 print_every=200, validate_every=100):\n",
        "        \"\"\"\n",
        "        :param experiment_name: Name of the experiment. A folder with the name 'experiments/experiment_name` will be\n",
        "                created with all the data associated to this run.\n",
        "\n",
        "        :param model: PyTorch model of the neural network\n",
        "\n",
        "        :param optimizer: PyTorch optimizer\n",
        "\n",
        "        :param print_every: How often should we print the loss during training (and send some training plots to\n",
        "                tensorboard)\n",
        "\n",
        "        :param validate_every: How often (after how many training iterations) should we evaluate the results on the\n",
        "                validation set (and send some validation plots to tensorboard)\n",
        "        \"\"\"\n",
        "        # Create attributes:\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_function = loss_function\n",
        "        self.loader_train = loader_train\n",
        "        self.loader_val = loader_val\n",
        "        self.validate_every = validate_every\n",
        "        self.print_every = print_every\n",
        "        self.device = device\n",
        "        self.dtype = dtype\n",
        "\n",
        "        # Create experiment directory:\n",
        "        exp_path = os.path.join('experiments', experiment_name)\n",
        "        os.makedirs(exp_path, exist_ok=True)\n",
        "\n",
        "        # Create Tensorboard logger:\n",
        "        self.writer = SummaryWriter(exp_path)\n",
        "\n",
        "    def train_model(self, epochs=1):\n",
        "        \"\"\"\n",
        "        - epochs: (Optional) An integer giving the number of epochs to train for\n",
        "        \"\"\"\n",
        "        model = self.model.to(device=self.device)  # move the model parameters to CPU/GPU\n",
        "        nb_iter_per_epoch = 0\n",
        "        for e in range(epochs):\n",
        "            for t, (input, target) in enumerate(self.loader_train):\n",
        "                model.train()  # put model to training mode\n",
        "                input = input.to(device=self.device, dtype=self.dtype)  # move to device, e.g. GPU\n",
        "                target = target.to(device=self.device, dtype=self.dtype)\n",
        "\n",
        "                prediction = model(input)\n",
        "                loss = self.loss_function(prediction, target)\n",
        "\n",
        "                # Zero out all of the gradients for the variables which the optimizer\n",
        "                # will update.\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # This is the backwards pass: compute the gradient of the loss with\n",
        "                # respect to each  parameter of the model.\n",
        "                loss.backward()\n",
        "\n",
        "                # Actually update the parameters of the model using the gradients\n",
        "                # computed by the backwards pass.\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Make sure that we apply a final activation if it was not done already:\n",
        "                if self.model.final_activation is None:\n",
        "                    #print(prediction.max().item(), prediction.min().item())\n",
        "                    prediction = torch.sigmoid(prediction)\n",
        "\n",
        "                # Compute metrics:\n",
        "                accuracy = compute_accuracy(prediction, target)\n",
        "                IoU = compute_IoU(prediction, target)\n",
        "\n",
        "                # Log some data to tensorboard:\n",
        "                self.writer.add_scalar('loss_train', loss.item(), t + e * nb_iter_per_epoch)\n",
        "                self.writer.add_scalar('IoU_train', IoU.item(), t + e * nb_iter_per_epoch)\n",
        "                self.writer.add_scalar('accuracy_train', accuracy.item(), t + e * nb_iter_per_epoch)\n",
        "\n",
        "                if t % self.print_every == 0:\n",
        "                    self.make_plots(input, prediction, target, t + e * nb_iter_per_epoch, \"predictions_train\")\n",
        "                    string1 = f'Epoch {e + 1}, iter {t}'\n",
        "                    string2 = f'Loss: {loss.item()}'\n",
        "                    print('{:<25s} ---> \\t{:<30s}'.format(string1, string2))\n",
        "\n",
        "                if t % self.validate_every == 0:\n",
        "                    self.evaluate_metrics_on_val_set(t + e * nb_iter_per_epoch)\n",
        "\n",
        "                # Increase counter:\n",
        "                if e == 0:\n",
        "                    nb_iter_per_epoch += 1\n",
        "\n",
        "    def evaluate_metrics_on_val_set(self, global_step=None):\n",
        "        # Set model to evaluation mode:\n",
        "        # this is very important because some types of layers (for example BatchNorm) behave differently\n",
        "        # during training and during evaluation.\n",
        "        self.model.eval()\n",
        "\n",
        "        # From now on, we make sure that torch does store data for computing gradients, since we won't\n",
        "        # update the parameters of the model during validation. This makes the computations faster and\n",
        "        # uses much less GPU memory.\n",
        "        with torch.no_grad():\n",
        "            # During validation, we accumulate these values across the whole dataset and then average at the end:\n",
        "            accuracy, IoU, loss = 0., 0., 0.\n",
        "            nb_iter = 0\n",
        "            for input, target in self.loader_val:\n",
        "                input = input.to(device=self.device, dtype=self.dtype)  # move to device, e.g. GPU\n",
        "                target = target.to(device=self.device, dtype=self.dtype)\n",
        "                prediction = self.model(input)\n",
        "                loss = loss + self.loss_function(prediction, target)\n",
        "\n",
        "                # Make sure that we apply a final activation if it was not done already:\n",
        "                if self.model.final_activation is None:\n",
        "                    prediction = torch.sigmoid(prediction)\n",
        "\n",
        "                accuracy = accuracy + compute_accuracy(prediction, target)\n",
        "                IoU = IoU + compute_IoU(prediction, target)\n",
        "                if nb_iter == 0:\n",
        "                    self.make_plots(input, prediction, target, global_step, name_figure=\"predictions_val\")\n",
        "                nb_iter += 1\n",
        "            \n",
        "            loss = loss / nb_iter\n",
        "            IoU = IoU / nb_iter\n",
        "            accuracy = accuracy / nb_iter\n",
        "            if global_step is not None:\n",
        "                # Log scores averaged over all the valid set (send them to tensorboard):\n",
        "                self.writer.add_scalar('loss_validation', loss.item(), global_step)\n",
        "                self.writer.add_scalar('IoU_validation', IoU.item(), global_step)\n",
        "                self.writer.add_scalar('accuracy_validation', accuracy.item(), global_step)\n",
        "            else:\n",
        "                # Print the results and return them:\n",
        "                print(\"Validation loss function: \", loss.item())\n",
        "                print(\"Validation IoU: \", IoU.item())\n",
        "                print(\"Validation accuracy: \", accuracy.item())\n",
        "                return loss.item(), IoU.item(), accuracy.item() \n",
        "\n",
        "    def make_plots(self, input, predictions, targets,\n",
        "                   step, name_figure=\"image_log\"):\n",
        "        # First, we need to move the data back to CPU:\n",
        "        input = input.cpu().detach().numpy()\n",
        "        predictions = predictions.cpu().detach().numpy()\n",
        "        targets = targets.cpu().detach().numpy()\n",
        "\n",
        "        # Then we create some plots\n",
        "        f, axes = plt.subplots(ncols=4, nrows=predictions.shape[0], figsize=(8, 8))\n",
        "        for ax in axes.flatten():\n",
        "            ax.axis('off')  # Delete axes\n",
        "        axes[0, 0].set_title(\"Input image\")\n",
        "        axes[0, 1].set_title(\"Yeast-cell\\nprediction\")\n",
        "        axes[0, 2].set_title(\"Ground truth\")\n",
        "        axes[0, 3].set_title(\"Pixels not\\ncorrectly classified\")\n",
        "        for btc in range(predictions.shape[0]):\n",
        "            axes[btc, 0].imshow(input[btc, 0], cmap='gray')\n",
        "            axes[btc, 1].imshow(predictions[btc, 0], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[btc, 2].imshow(targets[btc, 0], cmap='gray', vmin=0, vmax=1)\n",
        "            axes[btc, 3].imshow((predictions[btc, 0] > 0.5) == targets[btc, 0], cmap='seismic_r')\n",
        "        plt.tight_layout()  # Reduce padding between subplots\n",
        "\n",
        "        self.writer.add_figure(name_figure, f, step)  # Send the plot to tensorboard\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1xcFvNyDVcg-"
      },
      "source": [
        "##### Task 4.) Your first experiment!\n",
        "In the next block we create a UNet model, build the Adam optimizer that will take care of updating the parameters and then start the training. You can choose an `experiment_name` to be passed to the `Trainer` class, so that you will find all the data related to it in the folder `experiments/experiment_name`. \n",
        "\n",
        "**Remark about the number of epochs:** Observing the loss plots during the first epoch is already enough to see if there are some bugs in your implementation. Running scripts on Google Colab seems to take longer than on a local GPU, but after few epochs (max 5) you should already be able to draw your conclusions. On a local GPU, 10 epochs will take approximately one hour. In the bonus exercise, you can decide to let your best model train longer to see how good it can get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f3dTQLTxVcg-",
        "outputId": "731fa93a-7b21-44e3-96a3-522d497f8c13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "### NAME EXPERIMENT: first experiment with Soresen-Dice Loss ###\n",
        "\n",
        "from cvf20.models.unet import UNet\n",
        "\n",
        "# Build a basic UNet model:\n",
        "starting_model = UNet(\n",
        "     depth=5,\n",
        "     in_channels=1,\n",
        "     out_channels=1,\n",
        "     fmaps=(16, 32, 64, 128, 512, 1024),\n",
        "     dim=2,\n",
        "     scale_factor=2,\n",
        "     activation=nn.ReLU,\n",
        "     final_activation=nn.Sigmoid\n",
        ")\n",
        "\n",
        "# Build the optimizer:\n",
        "params = starting_model.parameters()\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.AdamW(params, lr=learning_rate)\n",
        "\n",
        "# Build the trainer with the Soresen-Dice loss you implemented:\n",
        "trainer = Trainer('first_exp_diceLoss', starting_model, optimizer, SorensenDiceLoss(),\n",
        "        train_loader, val_loader, dtype, device)\n",
        "\n",
        "# Start training:\n",
        "trainer.train_model(epochs=1)\n",
        "final_scores = trainer.evaluate_metrics_on_val_set()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, iter 0           ---> \tLoss: 0.09453822672367096     \n",
            "Epoch 1, iter 200         ---> \tLoss: 0.03892137110233307     \n",
            "Epoch 1, iter 400         ---> \tLoss: 0.01873360574245453     \n",
            "Validation loss function:  0.01788919046521187\n",
            "Validation IoU:  0.8388813734054565\n",
            "Validation accuracy:  0.9139347076416016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BI2LMxWbVchK"
      },
      "source": [
        "##### Task 5.) Have a look at what it was logged in Tensorboard!\n",
        "By running the following code block, as an output you will get a link that you can use to see tensorboard in a separate webpage (if you are using Google Colab). If instead you are using your local conda environment, then you can find tensorboard at [http://localhost:6006](http://localhost:6006).\n",
        "\n",
        "In Tensorboard, you will see two icons in the upper orange bar: `Scalars` (showing scores and value of the loss) and `Images`.\n",
        "\n",
        "If you are not happy with one experiment and you want to delete the data from Tensorboard, just delete the folder `experiments/experiment_name` containing its data (for example by running `!rm -r ./experiments/experiment_name`)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bZU1I2Qt_-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### RUN THIS CODE TO START TENSORBOARD ###\n",
        "LOG_DIR = './experiments'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aabKdORt_-w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbeddd56-a4bd-492b-a90c-86ec88b14975"
      },
      "source": [
        "### RUN THIS CODE IF YOU USE GOOGLE COLAB AND YOU WANT TO SEE TENSORBOARD IN ANOTHER WEBPAGE (output link) ###\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://ec1585f3.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dP8Yi6RlVchL",
        "colab": {}
      },
      "source": [
        "### RUN THIS CODE IF YOU USE GOOGLE COLAB AND YOU WANT TO SEE TENSORBOARD HERE IN THE NOTEBOOK ###\n",
        "# Remark: This method is less preferred because this embedded interface seems to be less responsive \n",
        "# and it does not allow to download images of the plots\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir experiments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "klPDRZcBVchh"
      },
      "source": [
        "##### Task 6.) Binary Cross Entropy Loss\n",
        "Now run another experiment using this different type of loss. With this loss function, sometimes the training works  better if you remove the final `Sigmoid` activation from the model and use the loss `torch.nn.BCEWithLogitsLoss`, which combines a Sigmoid layer and the `torch.nn.BCELoss` in one single class to avoid outputs with infinite values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CBrXIgtcapXe",
        "outputId": "86890353-87f3-4dec-c756-1606fd0abe30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "### NAME EXPERIMENT: second experiment with BCE Loss ###\n",
        "\n",
        "from cvf20.models.unet import UNet\n",
        "\n",
        "### Your code starts here (see first experiments above) ###\n",
        "\n",
        "# Build a basic UNet model:\n",
        "starting_model = UNet(\n",
        "     depth=5,\n",
        "     in_channels=1,\n",
        "     out_channels=1,\n",
        "     fmaps=(16, 32, 64, 128, 512, 1024),\n",
        "     dim=2,\n",
        "     scale_factor=2,\n",
        "     activation=nn.ReLU,\n",
        "     #final_activation=nn.Sigmoid\n",
        ")\n",
        "\n",
        "# Build the optimizer:\n",
        "params = starting_model.parameters()\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.AdamW(params, lr=learning_rate)\n",
        "\n",
        "# Build the trainer with the Soresen-Dice loss you implemented:\n",
        "trainer = Trainer('second_exp_BCELoss', starting_model, optimizer, nn.BCEWithLogitsLoss(), #SorensenDiceLoss\n",
        "        train_loader, val_loader, dtype, device)\n",
        "\n",
        "# Start training:\n",
        "trainer.train_model(epochs=1)\n",
        "final_scores = trainer.evaluate_metrics_on_val_set()\n",
        "\n",
        "### Your code ends here ###"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, iter 0           ---> \tLoss: 0.7117297649383545      \n",
            "Epoch 1, iter 200         ---> \tLoss: 0.2925015091896057      \n",
            "Epoch 1, iter 400         ---> \tLoss: 0.17424969375133514     \n",
            "Validation loss function:  0.22514590620994568\n",
            "Validation IoU:  0.8241847157478333\n",
            "Validation accuracy:  0.905179500579834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Z2hZaX_KQe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e38f0a0-dca4-4948-baeb-eb6009d5815c"
      },
      "source": [
        "### RUN THIS CODE IF YOU USE GOOGLE COLAB AND YOU WANT TO SEE TENSORBOARD IN ANOTHER WEBPAGE (output link) ###\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://ec1585f3.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zUlQU_dBKhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMHjTOXmt_-3",
        "colab_type": "text"
      },
      "source": [
        "**Dice Loss** performs slightly better than **Dice Loss** in terms of both loss stability (relatively small vs relatively large in terms of the absolute values of the loss function), as well as prediction accuracy (91.4% vs 90.5%). \n",
        "\n",
        "This is probably due to the fact that BCE Loss evaluates the class predictions for each pixel vector individually and then averages over all pixels, so effectively it asserts equal learning at all pixels in the image. This can be a problem if the target classes have unbalanced representation in the image, as training can be dominated by the most prevalent class. \n",
        "\n",
        "\n",
        "**Dice Loss** is better at semantic segmentation, especially when the image is dominated by an unproportional amount of one class vs the other, i.e. class imbalance. This is due to the fact the numerator is concerned with the common activations between our prediction and target mask, where as the denominator is concerned with the quantity of activations in each mask separately, which has the effect of \"normalizing\" the loss according to the quantity of the presence. Hence, it does not struggle learning from classes with lesser spatial representation in an image.   \n",
        "\n",
        "\n",
        "\n",
        "Do not forget to report the achived scores and comment them. You can also download some plots in tensorboard or take screenshots of the plots shown there to support your comments. You can either load these figures here in the notebook, point us to their path in your `.zip` submission file, or comment them in a seperate `LaTex` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsuqBKdHt_-3",
        "colab_type": "text"
      },
      "source": [
        "##### Task 7.) Add normalization layers\n",
        "Repeat the previous two experiments by adding normalization layers to the UNet model.\n",
        "\n",
        "For small mini-batch sizes or mini-batches that do not contain a representative distribution of examples from the training dataset, the differences in the standardized inputs between training and inference (using the model after training) can result in noticeable differences in performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYOct-v4vwXM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "ca0a6dbf-efc0-4be6-af82-31cfdd722428"
      },
      "source": [
        "### NAME EXPERIMENT: Repeating first experiment (Dice Loss) with normalisation layers (BatchNorm2d)\n",
        "\n",
        "from cvf20.models.unet import UNet\n",
        "\n",
        "# Build a basic UNet model:\n",
        "starting_model = UNet(\n",
        "     depth=5,\n",
        "     in_channels=1,\n",
        "     out_channels=1,\n",
        "     fmaps=(16, 32, 64, 128, 512, 1024),\n",
        "     norm_type = nn.BatchNorm2d,\n",
        "     dim=2,\n",
        "     scale_factor=2,\n",
        "     activation=nn.ReLU,\n",
        "     final_activation=nn.Sigmoid\n",
        ")\n",
        "\n",
        "# Build the optimizer:\n",
        "params = starting_model.parameters()\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.AdamW(params, lr=learning_rate)\n",
        "\n",
        "# Build the trainer with the Soresen-Dice loss you implemented:\n",
        "trainer = Trainer('first_exp_diceLoss_BatchNorm2d', starting_model, optimizer, SorensenDiceLoss(),\n",
        "        train_loader, val_loader, dtype, device)\n",
        "\n",
        "# Start training:\n",
        "trainer.train_model(epochs=1)\n",
        "final_scores = trainer.evaluate_metrics_on_val_set()\n",
        "\n",
        "### Your code ends here ###"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, iter 0           ---> \tLoss: 0.08947831392288208     \n",
            "Epoch 1, iter 200         ---> \tLoss: 0.024150997400283813    \n",
            "Epoch 1, iter 400         ---> \tLoss: 0.028715983033180237    \n",
            "Validation loss function:  0.018311655148863792\n",
            "Validation IoU:  0.9012771844863892\n",
            "Validation accuracy:  0.9490838050842285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yck5hy8b7o8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "a53f069f-23d1-4b10-b343-45397f60e950"
      },
      "source": [
        "### NAME EXPERIMENT: second experiment (BCE Loss) with normalization layers (BatchNorm2d)\n",
        "\n",
        "from cvf20.models.unet import UNet\n",
        "\n",
        "# Build a basic UNet model:\n",
        "starting_model = UNet(\n",
        "     depth=5,\n",
        "     in_channels=1,\n",
        "     out_channels=1,\n",
        "     fmaps=(16, 32, 64, 128, 512, 1024),\n",
        "     norm_type = nn.BatchNorm2d,\n",
        "     dim=2,\n",
        "     scale_factor=2,\n",
        "     activation=nn.ReLU,\n",
        "     #final_activation=nn.Sigmoid\n",
        ")\n",
        "\n",
        "# Build the optimizer:\n",
        "params = starting_model.parameters()\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.AdamW(params, lr=learning_rate)\n",
        "\n",
        "# Build the trainer with the Soresen-Dice loss you implemented:\n",
        "trainer = Trainer('second_exp_BCELoss_BatchNorm2d', starting_model, optimizer, nn.BCEWithLogitsLoss(), #SorensenDiceLoss\n",
        "        train_loader, val_loader, dtype, device)\n",
        "\n",
        "# Start training:\n",
        "trainer.train_model(epochs=1)\n",
        "final_scores = trainer.evaluate_metrics_on_val_set()\n",
        "\n",
        "### Your code ends here ###"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, iter 0           ---> \tLoss: 0.7331900596618652      \n",
            "Epoch 1, iter 200         ---> \tLoss: 0.27666032314300537     \n",
            "Epoch 1, iter 400         ---> \tLoss: 0.25325995683670044     \n",
            "Validation loss function:  0.24374431371688843\n",
            "Validation IoU:  0.8986448645591736\n",
            "Validation accuracy:  0.9470446109771729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTm4YQxyBNAu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "646ff981-af79-4d13-d074-c54a67b1fdf9"
      },
      "source": [
        "### RUN THIS CODE IF YOU USE GOOGLE COLAB AND YOU WANT TO SEE TENSORBOARD IN ANOTHER WEBPAGE (output link) ###\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://ec1585f3.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koajRNIFBekq",
        "colab_type": "text"
      },
      "source": [
        "By applying normalisation layers (torch.nn.BatchNorm2d), both models train faster and there are much less epoch needed for the predition accuracy to converge. The accuracy also improved significantly over the same epoch (both models achieves close to c. 95% accuracy with normalisation vs. c. 90-91% without normalisation layers). Also, the validation in the BEC Loss model becomes more stable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v-JJqVeOVchh"
      },
      "source": [
        "### c) Get creative!\n",
        "Now run your additional experiments and report your results using the same scheme described above. Don't forget to explain what you implemented and point us to your code (if it is not included in this notebook).\n",
        "\n",
        "You can find documentation for all the neural network layers implemented in PyTorch at [this link](https://pytorch.org/docs/stable/nn.html) (layer categories are on the right)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O49zJUwWt_-4",
        "colab_type": "text"
      },
      "source": [
        "##### Exp. 1: Description\n",
        "*Insert description here*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJWxfGsRt_-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### NAME EXPERIMENT: your experiment ###\n",
        "\n",
        "from cvf20.models.unet import UNet\n",
        "\n",
        "your_model = None\n",
        "\n",
        "# Build the optimizer:\n",
        "pass\n",
        "\n",
        "# Build the trainer:\n",
        "pass\n",
        "\n",
        "# Start training:\n",
        "pass\n",
        "final_scores = trainer.evaluate_metrics_on_val_set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYJ-E-rUt_-6",
        "colab_type": "text"
      },
      "source": [
        "##### Exp.1: Comments and Results\n",
        "*Insert description here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJrKhyoJt_-7",
        "colab_type": "text"
      },
      "source": [
        "##### Exp. 2: Description\n",
        "*Insert description here*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3vWnRZVt_-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### NAME EXPERIMENT: your experiment ###\n",
        "\n",
        "from cvf20.models.unet import UNet\n",
        "\n",
        "your_model = None\n",
        "\n",
        "# Build the optimizer:\n",
        "pass\n",
        "\n",
        "# Build the trainer:\n",
        "pass\n",
        "\n",
        "# Start training:\n",
        "pass\n",
        "final_scores = trainer.evaluate_metrics_on_val_set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPIepCbRt_-9",
        "colab_type": "text"
      },
      "source": [
        "##### Exp.2: Comments and Results\n",
        "*Insert description here*"
      ]
    }
  ]
}